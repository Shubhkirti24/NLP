{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering :\n",
        "- For ML dta must be in tabular form and numerical\n",
        "- for categorical : One hot encoding\n",
        "      pd.get_dummies(df, columns=[ 'name' ])\n",
        "  - not mentioning columns : automatically encode all non numeric features\n",
        "  - pre-processing (reduction -> reduce, converting to the base form)\n",
        "  - Vectorization\n",
        "  - Extract basic word features (length, hashtags, etc)\n",
        "  - POS tagging : part of speech tagging , I -> pronoun\n",
        "  - Named Entity recognition\n",
        "\n",
        "## Basic Feature Extraction :\n",
        "- Number of characters, including whitespaces\n",
        "- applying function :\n",
        "      df['new_col'] = df['old_col'].apply(function)\n",
        "- number of words : str.split(' ')\n",
        "- create a new function to apply function\n",
        "- Average word length\n",
        "      Special features (word.startswith(#))\n",
        "- other basic features, number of sentences, numbers, etc.\n",
        "\n",
        "## Readability tests :\n",
        "- english knowledge\n",
        "- primary to graduate level\n",
        "- mathematcial formula using the word, syllable and sentence count (fake news, spam detection)\n",
        "- eg: flesch reading ease, gunning fog index, simple measure of gobbledygook (SMOG), Dale-chall Score\n",
        "- Flesch :\n",
        "  - odlest, widely\n",
        "  - based on sentence length\n",
        "  - based on number of average syllables\n",
        "  - higher score -> easier to understand\n",
        "- Gunning:\n",
        "  - average sentence length\n",
        "  - uses % of complex words (3 or more syll)\n",
        "  - higher -> more difficult\n",
        "- using python\n",
        "  - from textatistic\n",
        "  - Textatistic(text).scores\n",
        "  - is dict\n",
        "\n",
        "## Tokenization and Lemmatization\n",
        "- Text from variety of sources\n",
        "- standardizing is important\n",
        "- Text preprocessing:\n",
        "  - Tokenization : splitting a string into token based on language. eg, words, sentences, punctuations\n",
        "  - expanding contracted words\n",
        "  - Library :\n",
        "         import spacy\n",
        "         model : spacy.load('en_core_web_sm')\n",
        "         doc = model(string)\n",
        "         tokens = [token.text for token in doc]\n",
        "\n",
        "  - Lemmatization : words into its base form\n",
        "  - am,are,is - be, n't - not, etc.\n",
        "  - spacy is done automatically\n",
        "        lemma = [token.lemma_ for token in doc]\n",
        "  - is/etc is converted to '-PRON-' meaning its a pronoun\n",
        "\n",
        "  - NLTK is good for sentence tokenization, spacy for words\n",
        "\n",
        "## Text cleaning:\n",
        "- isalpha(), isnumeirc(), using regex\n",
        "- stopwords, ignore : spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "## Part of speech tagging :\n",
        "- word sense disambiguation (bear/bear)\n",
        "- noun vs verb\n",
        "- sentiment analysis / question answering / fake news\n",
        "- assigning word/token its part of speech\n",
        "- using spacy :\n",
        "      pos = [(toke.text, token.pos_) for token in doc]\n",
        "  - accuracy depends\n",
        "  - 20 parts of speech\n",
        "\n",
        "## Named entity recognition :\n",
        "- search algorithms, question answering, news article classification\n",
        "- identifying and classifying\n",
        "- spacy:\n",
        "      ne = [(ent.text,ent.label_) for ent in doc.ents]\n",
        "   (15 different types of named entity)\n",
        "\n",
        "## Vectorization : creating a big bag of words:\n",
        "- data: tabular + numerical\n",
        "- converting text documents into numerical\n",
        "- extract word tokens, compute frequency,\n",
        "construct a word vector using frq anf vocab.\n",
        "\n",
        "- Using Sklearn :\n",
        "- create a text corpus using pd.series[]\n",
        "  - Countvectorizer :\n",
        "        from sklearn.feature_extraction.text import CountVectorizer\n",
        "        Vectorizer = CountVectorizer()\n",
        "  - generate matrix of 2D vectors:\n",
        "        bow_matrix = vectorizer.fit_transform(corpus)\n",
        "        bow_matrix.to_array()\n",
        "  - Convert bow_matrix into a DataFrame :\n",
        "        bow_df = pd.DataFrame(bow_matrix.toarray())\n",
        "    - Map the column names to vocabulary :\n",
        "        bow_df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "## BoW Naive Bayes Classifier :\n",
        "- Spam Filtering (Spam Vs Ham)\n",
        "  - Using CountVectrizer arguments :\n",
        "      lowercase, strip_accents('unicode','ascii', 'None')\n",
        "      stop_words ('english','list','None')\n",
        "      token_pattern (regex) / tokenizer : function\n",
        "    - No lemmatization\n",
        "    - Main job : matrix\n",
        "    - if words in test not in train, countvectorizer ignores those words\n",
        "- Training :\n",
        "      from sklearn.naive_bayes import MultinomialNB\n",
        "      clf = MultinomialNB()\n",
        "      clf.fit(x_train_bow, y_train)\n",
        "\n",
        "Project : Spam Filter\n",
        "\n",
        "## N-gram Models :\n",
        "\n",
        "- BoW representational problems (contex lost)\n",
        "- contiguous sequence of elements (BoW : n=1)\n",
        "- n=2, (a boy, boy lost,lost his), etc.\n",
        "- captures more context (not)\n",
        "- sentence completion, spelling correction, etc. It basically computes the probability of the words occuring together\n",
        "- CountVectorizer :\n",
        "      bigrams = Countvectorizer(ngram_range=(2,2))\n",
        "  - genreates unigrams, bigrams, trigrams\n",
        "- increasing curse of dimensionality\n",
        "\n",
        "## Building tf-idf document vectors :\n",
        "(Term Frequency inverse document frequency)\n",
        "- commonly uccuring words increases dimensions\n",
        "- Exclusivity, assigning more **weight** to word\n",
        "- example : jupiter vs universe, characterizing a document using that word\n",
        "- Automatically detect stop words, search, recommender systems\n",
        "- tf-idf : weight of the document should be proportional to its frequency and inversely prop to the number of document it occurs in.\n",
        "   - **Formula** weight = word freq in doc * log(number of docs/number of docs containing word)\n",
        "   - more weight, more characteristic\n",
        "   - scikit-learn :\n",
        "          from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "          vect = TfidfVectorizer()\n",
        "          tfidf_matrix = vect.fit_transform(corpus)\n",
        "          tfidf_toarray()\n",
        "    - weights are non-integer\n",
        "- magnitude of if-idf vector is always 1.\n",
        "\n",
        "## Cosine similarity :\n",
        "\n",
        "- Similarity between 2 vectors, i.e documents\n",
        "- Cosine similarity score\n",
        "- cos theta = A.B / magnitude\n",
        "- NLP cosine values bw 0 1 (as document vectors use non negative weights)\n",
        "- Cosine score ignores the mag of the vectors, therefore. robust to document length\n",
        "- Scikit learn :\n",
        "  - takes only 2D arrays as arguments\n",
        "        from sklearn.metrics.pairwise import cosine_similartiy\n",
        "        score = cosine_similarity ([a],[a])\n",
        "\n",
        "## Building a plot line based recommender :\n",
        "\n",
        "- Suggests movies based on overviews\n",
        "- recommender function :\n",
        "- ignores the highest similarity score of 1\n",
        "- Since the magnitude of tf-idf vector is always 1, the cosinbe score will always be equal to the dot product\n",
        "- Linear_kernel function (same, import linear_kernel)\n",
        "\n",
        "## Beyond n-grams, Word Embeddings :\n",
        "\n",
        "- synonyms : happy,joyous and sad would be vectorized similarly . h-j same score as h-s.\n",
        "- word embeddings : mapping words to an n-dimensional vector space, produced using deep learning and huge amounts of data. Similarity of words\n",
        "- complex relationships (king/queen, man/woman)\n",
        "- dependant on pretrained model\n",
        "\n",
        "- Spacy :\n",
        "      nlp = spacy.load('en_core_web_lg')\n",
        "- generate word vector for each token :\n",
        "      for token in doc: print(token.vector)\n",
        "- Word similarity score :\n",
        "      doc = nlp('happy sad joyous')\n",
        "      for token1 in doc:\n",
        "      for token2 in doc:\n",
        "      print(token1.text, token2.text, token1.similarity(token2))\n",
        "- Document similarity :\n",
        "  - between sentences as well\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lgkQnuq7EeW8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qGBJzahI-GB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lectures 3 Feature Engineering :\n",
        "\n",
        "## Count based models :\n",
        "- Vectorization, convert the words into vectors\n",
        "- Combine the vectors to create a vectorspace\n",
        "- unique words creates a vectorspace\n",
        "- BOW : loses context, semantics\n",
        "- tf-idf, n grams\n",
        "\n",
        "\n",
        "## Word vectors :\n",
        "- Idea of distributed representation / dependance on other words (linear algebra)\n",
        "- representing word vectors as continuous mutidimensional floating point numbers\n",
        "- semantically similar words will be mapped nearer (Realtions : singular, plural, gender, other relations)\n",
        "- Word2Vec Model, the contenxt for each word is in its nearby words (Google) : distributional hypothesis\n",
        "-<b> Word embedding : </b> The transformation of word to vectors is word embedding\n",
        "- ** Countinuous bag of words + Skip Gram**\n",
        "- CBOW :Learns embedding by predicting the current word based on the context (faster, has more accuracy for frequent words)\n",
        "- Skip gram : learns embeddings by preding surrounding words based on the current word (works well with small amt of data, better with rare words/ phrases)\n",
        "\n",
        "### CBOW :\n",
        "[Link](https://iu.instructure.com/courses/2165940/files/160988843/download?wrap=1)\n",
        "- weight is assigned to the current word using the context window\n",
        "- Word2Vec is unsupervised - no label\n",
        "- Keras :\n",
        "      from keras.preprocessing import text\n",
        "      from keras.utils import np_utils\n",
        "      from keras.preprocessing import sequence\n",
        "- Preprocess :\n",
        "      from nltk.tokenize import sent_tokenize\n",
        "      from nltk.tokenize import RegexpTokenizer\n",
        "      from nltk.corpus import stopwords\n",
        "      from nltk.corpus import gutenberg\n",
        "      alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
        "\n",
        "- function :\n",
        "      def norm(text):\n",
        "        norm_text = []\n",
        "        tokenizer = RegexpTokenizer('[a-zA-Z]+')\n",
        "        tokens_sentences = [tokenizer.tokenize(t) for t in\n",
        "        sent_tokenize(text)]\n",
        "        stop_words = stopwords.words('english')\n",
        "        for s in tokens_sentences:\n",
        "          w_norm = []\n",
        "          for w in s:\n",
        "            if not w.lower() in stop_words:\n",
        "              w_norm.append(w.lower())\n",
        "              norm_text.append(' '.join(w_norm))\n",
        "      return(norm_text)\n",
        "\n",
        "- Create a vocab :\n",
        "  tokenizer :\n",
        "      tokenizer = text.Tokenizer()\n",
        "      tokenizer.fit_on_scale(corpus)\n",
        "      word2id = tokenizer.word_index\n",
        "    - lower integer : more frequent word\n",
        "  \n",
        "  - generate context_window, target word pairs\n",
        "  - **Yield : generator iterator**\n",
        "  - code :\n",
        "\n",
        "      def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "        context_length = window_size*2\n",
        "        for words in corpus:\n",
        "          sentence_length = len(words)\n",
        "          for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word = []\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            context_words.append([words[i]\n",
        "              for i in range(start, end)\n",
        "                if 0 <= i < sentence_length and i != index])\n",
        "                  label_word.append(word)\n",
        "          x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
        "          y = np_utils.to_categorical(label_word, vocab_size)\n",
        "        yield (x, y)\n",
        "\n",
        "  - Create a sequential model :\n",
        "        vocab_size = len(word2id)\n",
        "        embed_size = 100\n",
        "        window_size = 2\n",
        "        \n",
        "        cbow = Sequential()\n",
        "        cbow.add(Embedding(input_dim=vocab_size,output_dim=embed_size,\n",
        "        input_length=window_size*2))\n",
        "        \n",
        "        cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=()))\n",
        "        \n",
        "        cbow.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "        \n",
        "        cbow.compile(loss='categorical_crossentropy', optimizer=)\n",
        "\n",
        "- or epoch in range(1, 3): # 2 epoch for demo. Use more epoch\n",
        "      loss = 0.\n",
        "      i = 0\n",
        "      for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "      i += 1\n",
        "      loss += cbow.train_on_batch(x, y)\n",
        "      if i % 100000 == 0:\n",
        "      print('Processed {} (context, word) pairs'.format(i))\n",
        "      print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "      print()\n",
        "\n",
        "- Get Word Embeddings :\n",
        "      cbow.get_weights()[0]\n",
        "      from sklearn.metrics.pairwise import euclidean_distances\n",
        "      distance_matrix = euclidean_distances(weights)\n",
        "\n",
        "      Similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-\n",
        "      1].argsort()[1:6]+1]\n",
        "      for search_term in ['alice', 'queen', 'rabbit']}\n",
        "\n",
        "- Visualize :\n",
        "      from sklearn.manifold import TSNE\n",
        "      import pylab as plt\n",
        "      words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "      words_ids = [word2id[w] for w in words]\n",
        "      word_vectors = np.array([weights[idx] for idx in words_ids])\n",
        "      print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)\n",
        "      \n",
        "      tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\n",
        "      np.set_printoptions(suppress=True)\n",
        "      T = tsne.fit_transform(word_vectors)\n",
        "      labels = words\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.scatter(T[:, 0], T[:, 1], c=\"steelblue\", edgecolors=\"k\", s= 40)\n",
        "      \n",
        "      for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "      plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points', fontsize = 16)\n",
        "\n",
        "\n",
        "## Vector based models :\n",
        "[Link](https://iu.instructure.com/courses/2165940/files/160988885/download?wrap=1)\n",
        "\n",
        "- BOW Count based : TF, IDF, N-grams\n",
        "- Prediciton based : Distributed representations word embedding vector, Word2vec, FastText, dense representation in multiple dimensions\n",
        "- Word2Vec : has a small window, unable to learn from global frequency, smallest unit : word, negative sampling, each training sample only updates a % of model's weight\n",
        "- GloVe : word, frequent occ : carry additional info,\n",
        "(in a way count based model), no window\n",
        "- Both face issues with unknown words, remedy : treat all words as out of vocab\n",
        "- FastText : smallest unit : character (wh,whe,ere), each word as a bag of character ingrams\n",
        "- Sentence embedding : Doc2Vec (skip through models):\n",
        "  - Unsupervised : Skip-thoughts, quick-thoughts\n",
        "  - supervised : InferSent\n",
        "\n",
        "\n",
        "# Week 4: Similarity\n",
        "\n",
        "##  Distance Metrics Summary :\n",
        "1. Euclidean distance - the diagonal line - the shortest path between two points, also known as L2 Norm and Pythagorean metric.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html\n",
        "\n",
        "\n",
        "\n",
        "2. Manhattan distance -  the total sum |absolute value| of the differences between the x-coordinates and y-coordinates. Also known as  L1 distance or L1 norm, or city block distance.\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cityblock.html\n",
        "\n",
        "\n",
        "\n",
        "3. Minkowski distance is a generalization of Euclidean and Manhattan distance, and it defines the distance between two points in a normalized vector space.  When p = 1, the distance is Manhattan; when p = 2, the distance is Euclidean.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html\n",
        "\n",
        "\n",
        "4. Jaccard Similarity Coefficient defines similarity between finite sets as the quotient (a result obtained by dividing one quantity by another) of their intersection [dividend] and their union [divisor].\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jaccard.html\n",
        "\n",
        "\n",
        "- Jaccard index /Similarity Coefficient is between 0 and Closer to 1 - more similar!\n",
        "  \n",
        "  - Jaccard distance measures dissimilarity between 2 sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1.\n",
        "  - Closer to 1 - more distant!\n",
        "  - jaccard distance formula: 1 minus Jaccard index\n",
        "  - Lemmatization is necessary for jacard index (reducing to the root words)\n",
        "\n",
        "        Defined as intersection over union:\n",
        "        # Jaccard index (similarity) + jaccard distance (dissimilarity) = 1\n",
        "\n",
        "\n",
        "\n",
        "5.  Cosine distance measures the degree of angle between two documents/vectors. Dividend is a dot product of two vectors, divisor is a product of Euclidean normsLinks to an external site. for each vector\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
        "\n",
        "  - The value close to 1 indicates very high similarity between the two vectors/documents\n",
        "  \n",
        "  - Document is converted to a vector (Rn) where n is the number of unique words and each element has a value associated with a word (e.g. TF, TFIDF, CBOW). Note - common frequent words will influence the similarity score\n",
        "\n",
        "          Have a corpus with 2 sentences :\n",
        "\n",
        "          corpus = [sen_1,sen_2]\n",
        "\n",
        "          vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "          t = vectorizer.fit_transform(corpus)\n",
        "          modelt = t.to_array()\n",
        "\n",
        "          # cosine similarity :\n",
        "\n",
        "          from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "          cosine_similarity(model)\n",
        "\n",
        "          # convert the cosine similarity into a df:\n",
        "\n",
        "          d = pd.DataFrame(get_cosine_sim(corpus),index = ['Doc1','Doc2'])\n",
        "          d.columns = ['Doc1','Doc2']\n",
        "\n",
        "- Cosine vs Jaccard similarity:\n",
        "\n",
        "  - Jaccard takes unique set of words into consideration, cosine takes the total length of the vectors\n",
        "  - **For Jaccard, word repetitions would'nt make a difference, for Cosine, the length of the vector would change if the word was repeated**\n",
        "\n",
        "\n",
        "## Document clustering using unsupervised learning:\n",
        "\n",
        "- an unsupervised learning to group data points(documents) into groups or clusters\n",
        "- 2 types:\n",
        "  - Partisional : division into non-overlapping groups\n",
        "  - Hierarchical : nested clusters are formed as an hierarchical tree, futher divided into:\n",
        "    - Agglomerative Clustering (Bottom up), seperate nodes at the bottom\n",
        "    - Divisive : Top down -> the most heterogenous clusters are divided into 2\n",
        "\n",
        "- Agglomerative clustering:\n",
        "  - Measuring dissimilarity in the clusters:\n",
        "    1. Max / complete linkage: computes all pairwise similarites between doc1 and doc2, then chooses the largest value, tends to produce more compact clusters\n",
        "    2. Min / Single linkage: pairwise, considers the smallest value, lose clusters\n",
        "    3. Mean / Average linkage: pairwise, considers the average of the dissimilarities\n",
        "    4. Ward Linkage: Minimizes total within cluster variance\n",
        "\n",
        "\n",
        "- Pairwise document similarity: if we have C documents in a corpus, we will end up with c by c matrix\n",
        "\n",
        "      # PLotting the dendogram:\n",
        "\n",
        "      from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "      similarity_matrix = cosine_similarity(tv_matrix)\n",
        "\n",
        "      # using the tf-idf scores from the normalised corpus:\n",
        "\n",
        "      from scipy.cluster.heirarchy import dendogram,linkage\n",
        "\n",
        "      Z = linkage(similarity_matrix,'ward')\n",
        "\n",
        "      pd.DataFrame (Z, columns=['Document\\Cluster1', 'Document\\Cluster2', 'Distance','Cluster Size'], dtype='object')\n",
        "\n",
        "      # plotting\n",
        "\n",
        "      plt.figure(figsize=(8,3))\n",
        "      plt.title('')\n",
        "      plt.xlabel('Data point')\n",
        "      plt.ylabel('Distance')\n",
        "\n",
        "      dendogram(Z)\n",
        "\n",
        "      plt.axhline(y=1.0, c='k',ls='--',lw=0.5)\n",
        "      plt.show()\n",
        "\n",
        "      # by adding labels to the documents\n",
        "\n",
        "      corpus_df = pd.DataFrame({'Document':corpus , 'Category':labels})\n",
        "      corpus_df = corpus_df[['Document','Category']]\n",
        "\n",
        "      from scipy.cluster.heirarchy import fcluster\n",
        "\n",
        "      max_dist = 1.0\n",
        "      cluster_labels = fcluster(Z, max_distance, criterion = 'distance')\n",
        "\n",
        "      cluster_labels = pd.dataFrame (cluster_labels, columns=['ClusterLabel'])\n",
        "      pd.concat([corpus_df,cluster_labels], axis = 1)\n",
        "\n",
        "## Topic Modeling:\n",
        "\n",
        "- Extracting key themes from a corpus of documents\n",
        "- Each topic is represented as a collection of words\n",
        "- **Latent Dirichlet Allocation (LDA):**\n",
        "  - LDA is a probabilistic model used for topic modeling, particularly in the field of natural language processing (NLP).\n",
        "  - Its primary goal is to discover topics within a collection of documents and assign documents to one or more of these topics.\n",
        "  - LDA assumes that each document is a mixture of various topics, and it aims to estimate the topic proportions for each document and the word distribution for each topic.\n",
        "  - LDA is an unsupervised technique and does not involve any classification or discrimination tasks. Its purpose is to reveal the underlying thematic structure of a document corpus.\n",
        "\n",
        "- Using Scikit learn to generate the topics (so we dont have to specify them manually)\n",
        "\n",
        "- Document-term matrix is divided into :\n",
        "  - Document - topic matrix (feature matrix)\n",
        "  - Topic-term matrix (potential topics in the corpus)\n",
        "\n",
        "        cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "        vocab = cv.get_feature_names()\n",
        "        cv_matrix = cv.fit_transform(norm_corpus)\n",
        "\n",
        "        from sklearn.decomposition import LatentDirichletAllocation\n",
        "        \n",
        "        # n_components = number of topics, the number of topics is less, dimesionality reduction happens\n",
        "        \n",
        "        lda = LatentDirichletAllocation(n_components=3, max_iter=10000,random_state=0)\n",
        "        \n",
        "        dt_matrix = lda.fit_transform(cv_matrix)\n",
        "        \n",
        "        features = pd.DataFrame(dt_matrix, columns=['T1', 'T2', 'T3'])\n",
        "\n",
        "        tt_matrix = lda.components_\n",
        "        for topic_weights in tt_matrix:\n",
        "          topic = [(token, weight) for token, weight in\n",
        "        zip(vocab, topic_weights)]\n",
        "        \n",
        "        topic = sorted(topic, key=lambda x: -x[1])\n",
        "        topic = [item for item in topic if item[1] > 0.6]print(topic)\n",
        "        print()\n",
        "\n"
      ],
      "metadata": {
        "id": "gR-4FI4W1EVr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_bnho0y-H0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP With SpaCY\n",
        "\n",
        "## Basics\n",
        "- Insights from unstructured data\n",
        "- statistics, machine learning and deep learning\n",
        "- Ner, sentiment analysis, text generation\n",
        "- spaCy, information extraction (spacy.io)\n",
        "- NLP object stores the processed text in the doc object\n",
        "- tokenization :\n",
        "\n",
        "      import spacy\n",
        "      nlp = spacy.load('en_core_web_sm')\n",
        "      text = ''\n",
        "      doc = nlp(text)\n",
        "      print [token.text for token in doc]\n",
        "\n",
        "- mutiple data structures to represent text:\n",
        "  - Doc (linguistic annotations of text)\n",
        "  - Span (a slice from a doc object)\n",
        "  - Token (an individual token)\n",
        "\n",
        "- Pipeline components :\n",
        "  - Tokenizer, tagger lemmatizer, entity recognizer\n",
        "  - sentence segmentation : uses DependencyParser\n",
        "        for sent in doc.sents:\n",
        "        print(sent.text)\n",
        "  - Lemmatisation :\n",
        "        print ([token.text, token.lemma_ for token in doc])\n",
        "\n",
        "        document = nlp(text)\n",
        "        tokens = [token.text for token in document]\n",
        "        \n",
        "        # Append the lemma for all tokens in the document\n",
        "        lemmas = [token.lemma_ for token in document]print(\"Lemmas:\\n\", lemmas, \"\\n\")\n",
        "        \n",
        "        # Print tokens and compare with lemmas list\n",
        "        print(\"Tokens:\\n\", [token.text for token in document])\n",
        "\n",
        "        # Generating a documents list of all Doc containers\n",
        "        documents = [nlp(text) for text in texts]\n",
        "        \n",
        "        # Iterate through documents and append sentences in each doc to the sentences list\n",
        "        sentences = []\n",
        "        for doc in documents:\n",
        "        sentences.append([s for s in doc.text])\n",
        "        \n",
        "        # Find number of sentences per each doc container\n",
        "        print([len(s) for s in sentences])\n",
        "\n",
        "        sentences = [[sent for sent in doc.sents] for doc in documents]\n",
        "  \n",
        "## Linguistic features in SpaCy\n",
        "\n",
        "- POS tagging(part of speech) : verb, noun, adj, adv, conj\n",
        "  - to confirm the meaning of the words\n",
        "  - spacy.explain()\n",
        "        print(\n",
        "          [(token.text, token.pos_, spacy.explain(token.pos_))\n",
        "          for token in nlp(sent)]\n",
        "          )\n",
        "- Named Entity recog\n",
        "  - Person, org, gpe(geop-political location), loc (eg. mountain ranges), date, time\n",
        "  - doc.ents(), label : .label_\n",
        "        print(\n",
        "          [(ent.text, ent.start_char, ent.end_char, ent,label_)\n",
        "          for ent in doc.ents]\n",
        "          )\n",
        "  - Alternate approach :\n",
        "        print (\n",
        "          [(token.text, token.ent_type_)\n",
        "          for token in doc]\n",
        "          )\n",
        "  - DisplaCy : visualizer\n",
        "        import spacy\n",
        "        from spacy import displacy\n",
        "        text = ''\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        doc = nlp(text)\n",
        "\n",
        "        display.serve(doc, style='ent')\n",
        "\n",
        "\n",
        "- Importance of POS :\n",
        "  - word sense disambiguation\n",
        "  - dependancy parsing :\n",
        "    - explores a sentence syntax, links between two tokens, ressults in a tree\n",
        "    - dependency label describes the tyoe of syntactic relation between 2 tokens\n",
        "    (nominal subject, root, determiner, direct object, auxiliary)\n",
        "  - Displaying a dependency tree:\n",
        "        doc = nlp(text)\n",
        "        spacy.displacy.serve(doc, style='dep')\n",
        "      - parent -> dependent\n",
        "            token.text, token.dep_, spacy.explain(token.dep_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Introduction to word vectors :\n",
        "\n",
        "- word vectors, word embeddings are numercial representations of text data\n",
        "- Doesn't help the computer to understand the meaning od the sentence, if the word embeddings are different, hence the model is oblivious to context and semantics\n",
        "- Adding pre-defined number of dimensions to the word vectors (eg: living being, feline, human, gender, royalty, verb, plural, etc)\n",
        "- Multiple approaches : Word2Vec, Glove, fastText, transformer based approaches\n",
        "- spacy models with word vectors :\n",
        "  - en_core_web_md / lg : how to check\n",
        "        nlp.meta[\"vectors\"]\n",
        "  - we can inly use the word vectors that exist in the model's vocab\n",
        "        nlp.vocab\n",
        "        like_id = nlp.vocab.strings['like']\n",
        "        nlp.vocab.vectors[like_id]\n",
        "        (can be used to access the word vectors using it's id)\n",
        "\n",
        "## Visualizing word vectors and similar context :\n",
        "- Using a scatter plot to visualize how the word vectors are grouped\n",
        "- extracting the principal components using PCA\n",
        "- using matplotlib, spacy, scikitlearn\n",
        "      import matplotlib.pyplot as plt\n",
        "      from sklearn.decomposition import PCA\n",
        "      import numpy as np\n",
        "\n",
        "      # extract the word vectors for a given list of words and stack them vertically\n",
        "\n",
        "      words = ['','']\n",
        "      word_vectors = np.vstack(\n",
        "        [nlp.vocab.vectors [nlp/vocab.strings[w]] for w in words]\n",
        "      )\n",
        "\n",
        "      # extract 2 principal components and project them into 2d space\n",
        "\n",
        "      pca = PCA(n_components = 2)\n",
        "      word_vectors_transformed = pca.fit_transform(word_vectors)\n",
        "\n",
        "      # visualising the scatter plot\n",
        "      plt.figure(figsize = (10,8))\n",
        "      plt.scatter(word_vectors_transformed[:,0],\n",
        "      word_vectors_tranformed[:, 1])\n",
        "\n",
        "      # adding words to the plot :\n",
        "\n",
        "      for word, coord in zip(words, word_vectors_transformed):\n",
        "        x,y = coord\n",
        "        plt.text(x,y,word, size=10)\n",
        "      plt.show()\n",
        "\n",
        "- Analogies and vector operations :\n",
        "  - a word analogy is a sematic relationship between a pair of words\n",
        "  - word embeddings generate analogies such as gender and tense, queen - woman + man = king\n",
        "\n",
        "  - finding similar words (semantically) in the vocab :\n",
        "        K-means :\n",
        "        import numpy as np\n",
        "        import spacy\n",
        "        nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "        word = 'covid'\n",
        "\n",
        "        most_similar_words = nlp.vocab.vectors.**most_similar** (\n",
        "          [nlp.vocab.vectors[nlp.vocab.string[word]]],n=5\n",
        "        )\n",
        "\n",
        "        words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
        "\n",
        "        print(words)\n",
        "    \n",
        "  - Semantic similarity help us categorize text into predefined categories or to detect relevant text / flag duplicate content\n",
        "    - finding similarity scores : cosine similarity\n",
        "          token1 = doc[2]\n",
        "          token2 = doc[3]\n",
        "          similarity = round(token1.similarity(token2),3)\n",
        "    - similarly spacy can calculate for 'span' object (span of a doc object)\n",
        "          span1 = doc1[1:], span2 = doc2[1:]\n",
        "          span1.similarty(span2)\n",
        "    - doc similarity : same, doc1.similarity\n",
        "  - getting contextually similar sentences\n",
        "         sentences = nlp(sentences)\n",
        "         keyword = nlp(word)\n",
        "\n",
        "         for i, sentence in in enumerate(sentences.sents):\n",
        "          print(\n",
        "            f\" Similarity score with sentence {i+1} :\n",
        "            round(sentence.similarity(keyword),5)\n",
        "          )\n",
        "\n",
        "## Spacy Pipelines\n",
        "\n",
        "- creating spacy pipelines / adding them to an existing pipeline:\n",
        "  - Adding pipes : sentence segmentation for a document with 10k sentences , long and time consuming (**sentencizer**)\n",
        "  - when using an existing model on the text, the whole nlp pipeline gets activated\n",
        "        doc = en_core_sm_nlp(text)\n",
        "  - A better method is to create a blank spacy model so that only the sentence segmentation part of the pipeline runs\n",
        "        # create a blank model and add a 'sentencizer' pipe\n",
        "        blank_nlp = spacy.blank('en')\n",
        "        blank_nlp.add_pipe('sentencizer')\n",
        "        doc = nlp(text)\n",
        "  - Analyzing pipeline components to check whether any attributes are not set\n",
        "    - nlp.analyze_pipes():\n",
        "      - attributes they set on the doc and token\n",
        "      - scores produced training\n",
        "      - shows warnings is the component values are not set\n",
        "            nlp = spacy.load('en_core_web_sm')\n",
        "            analysis = nlp.analyze_pipes(pretty=True)\n",
        "\n",
        "### EntityRuler\n",
        "\n",
        "- lets us add entity to doc.ents, or can be used on its own\n",
        "- using dictionary for patterns\n",
        "  - Phrase entity patterns (for exact string matches): {'label':'ORG', 'pattern':'Microsoft'}\n",
        "  - Token entity patterns (with one dictionary describing one token(list)) : {'label : 'GPE', 'pattern' : [{\"Lower\":'san'},{'Lower': 'francisco'}]}\n",
        "        nlp = spacy.blank('en')\n",
        "        entity_ruler = nlp.add_pipe('entity_ruler')\n",
        "        patterns = [{'label':'ORG', 'pattern':'Microsoft'},{'label : 'GPE', 'pattern' : [{\"Lower\":'san'},{'Lower': 'francisco'}]}]\n",
        "        entity_ruler.add_patterns(patterns)\n",
        "\n",
        "        doc = nlp(text)\n",
        "        print([(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "        # adding the 'before' will add the new pattern if there is no overlap\n",
        "        ruler = nlp.add_pipe('entity_ruler', before ='ner')\n",
        "\n",
        "\n",
        "### RegEx / matcher with Spacy\n",
        "- for instance :\n",
        "      import re\n",
        "      pattern = r\"((\\d){3} - (\\d){3} - (\\d){4})\"\n",
        "\n",
        "      iter_matches = re.finditer(pattern, text)\n",
        "\n",
        "      for match in phones:\n",
        "      start_char = match.start()\n",
        "      end_char = match.end()\n",
        "\n",
        "- with spacy\n",
        "  - patterns =\n",
        "        [{\"label\":'Phone_number'}, 'pattern':[{'SHAPE':'ddd'},{'ORTH':'-'},{'SHAPE':'ddd'},{'ORTH':'-'},{'SHAPE':'dddd'}]]\n",
        "\n",
        "- Alternative to regex : Matcher class\n",
        "\n",
        "      import spacy\n",
        "      from spacy.matcher import Matcher\n",
        "      nlp = spacy.load('en_core_web_sm')\n",
        "      doc = nlp(text)\n",
        "      \n",
        "      matcher = Matcher(nlp.vocab)\n",
        "\n",
        "      pattern = [{\"Lower\":'good'},{'Lower'{'IN':['eveninng','morning']}}]\n",
        "\n",
        "      matcher.add('morning_greeting',[pattern])\n",
        "      matches = matcher(doc)\n",
        "\n",
        "      for match_id, start, end in matches :\n",
        "        print('start_token:', start, \"|End token:\", end, \"|Matched Text\",\n",
        "        doc[start:end])\n",
        "\n",
        "  - Allows the patterns to be more expressive (IN / NOT IN)\n",
        "  - Phrase matcher helps tp match long phrases\n",
        "\n",
        "        from spacy.matcher import PhraseMatcher\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "        # can add attr = 'Lower' (to match words), 'SHAPE' (101.12.2)\n",
        "\n",
        "        terms = ['Bill Gates']\n",
        "\n",
        "        patterns = [nlp.make_doc(term) for term in terms]\n",
        "        matcher.add('PeopleOfInterest',patterns)\n",
        "\n",
        "## Customiziing\n",
        "- Not seen during training (# in twitter, medical data)\n",
        "  - can't be classfied accurately using spacy's NER models\n",
        "  - Training spacy models : common entities could be different to the specialised ones\n",
        "\n",
        "- Data Preparation :\n",
        "  1. Annotate and prepare input data\n",
        "  2. Initialize the model weights\n",
        "  3. predict using current weights\n",
        "  4. compare prediction with correct answers\n",
        "  5. use optimizer to calculate weights that improve model performance\n",
        "  6. update weights\n",
        "  7. repeat 3\n",
        "\n",
        "- Annotated data : has to be stored as a dictionary\n",
        "      {\n",
        "        'sentence' : text,\n",
        "        'entities' : {\n",
        "          'label' : 'Medicine',\n",
        "          'value' : 'neuraminidaise inhibitors'\n",
        "        }\n",
        "      }\n",
        "- Training data :\n",
        "      [\n",
        "        ('I will visit you in Austin', {'entities':[(20,26,'GPE')]})\n",
        "      ]\n",
        "\n",
        "- We can't put raw data directly into the model, we need to create an example object\n",
        "\n",
        "      import spacy\n",
        "      from spacy.training import Example\n",
        "      nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "      doc = nlp('I Live in Austin')\n",
        "      annotations = {'entities': [(20,26,'GPE')]}\n",
        "\n",
        "      example_sentence = Example.from_dict(doc,annotations)\n",
        "      print(example_sentence.to_dict())\n",
        "\n",
        "\n",
        "\n",
        "      # for instance\n",
        "      text = \"A patient with chest pain had hyperthyroidism.\"\n",
        "      entity_1 = \"chest pain\"\n",
        "      entity_2 = \"hyperthyroidism\"\n",
        "      \n",
        "      # Store annotated data information in the correct format\n",
        "      annotated_data = {\"sentence\": text, \"entities\": [{\"label\": 'SYMPTOM', \"value\": 'chest pain'}, {\"label\": 'DISEASE', \"value\": 'hyperthyroidism'}]}\n",
        "      \n",
        "      # Extract start and end characters of each entity\n",
        "      entity_1_start_char = text.find(entity_1)\n",
        "      entity_1_end_char = entity_1_start_char + len(entity_1)\n",
        "      entity_2_start_char = text.find(entity_2)\n",
        "      entity_2_end_char = entity_2_start_char + len(entity_2)\n",
        "      \n",
        "      # Store the same input information in the proper format for training\n",
        "      training_data = [(text, {\"entities\": [(entity_1_start_char,entity_1_end_char,\"SYMPTOM\"),\n",
        "                                      (entity_2_start_char,entity_2_end_char,\"DISEASE\")]})]\n",
        "      print(training_data)\n",
        "\n",
        "      #\n",
        "\n",
        "      example_text = 'A patient with chest pain had hyperthyroidism.'training_data = [(example_text, {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n",
        "      \n",
        "      all_examples = []\n",
        "      \n",
        "      # Iterate through text and annotations and convert text to a Doc container\n",
        "      \n",
        "      for text, entities in training_data:\n",
        "      doc = nlp(text)\n",
        "      \n",
        "      # Create an Example object from the doc contianer and annotations\n",
        "      \n",
        "      example_sentence = Example.from_dict(doc, entities)\n",
        "      print(example_sentence.to_dict(), \"\\n\")\n",
        "      \n",
        "      # Append the Example object to the list of all examples\n",
        "      \n",
        "      all_examples.append(example_sentence)\n",
        "      \n",
        "      print(\"Number of formatted training data: \", len(all_examples))\n",
        "\n",
        "  \n",
        "- Training spacy model for NER task\n",
        "  1. Annotate and prepare input data\n",
        "  2. Disable other pipeline components\n",
        "  3. Train a model for a few epochs\n",
        "  4. Evaluate model performance\n",
        "\n",
        "\n",
        "- Disabling all other pipeline components:\n",
        "      other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "      nlp.disable_pipes(*other_pipes)\n",
        "\n",
        "- Creating 'Optimizer' object to update the model weights:\n",
        "      optimizer = nlp.create_optimizer()\n",
        "\n",
        "      losses = {}\n",
        "\n",
        "      for i in range(epochs):\n",
        "        random.shuffle(training.data)\n",
        "\n",
        "        for text, annotation in training_data:\n",
        "          doc = nlp.make_doc(text)\n",
        "          example = Example.from_dict(doc, annotation)\n",
        "          nlp.update([example],sgd = optimizer, losses=losses)\n",
        "\n",
        "- Saving a trained model :\n",
        "\n",
        "      ner = nlp.get_pipe('ner')\n",
        "      ner.to_disk('<ner model name>')\n",
        "\n",
        "- Load the saved model :\n",
        "      ner = nlp.create_pipe('ner')\n",
        "      ner.from_disk('<ner model name>')\n",
        "      ner.add_pipe(ner, '<ner model name>')      \n"
      ],
      "metadata": {
        "id": "v3nd2x0E-Jud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 5 : Text classification\n",
        "\n",
        "- Text classification into given set of labels\n",
        "- Supervised,using training data\n",
        "- News, sentiment analysis (opinion mining), email filtering\n",
        "- Types on the basis of content:\n",
        "  - content based : priority is given to topic weights\n",
        "  - request based : user behaviour\n",
        "\n",
        "- Automations:\n",
        "  - Supervised (Classification and Regression) : prelabled data\n",
        "  - Unsupervised : Based on pattern mining and finding latent structures in data\n",
        "\n",
        "- Classification techniques:\n",
        "(characteristic : Low frequency highly dimensional data)\n",
        "  - Decision Trees\n",
        "  - Pattern (Rule based)\n",
        "  - SVM : optimal boundaries\n",
        "  - Bayesian (Generative) Classifiers : probabilistic classifier based on modeling the underlying word features in different classes\n",
        "\n",
        "### Mutltinomial Naive Bayes:\n",
        "- Assumption: Probabilities of occurrence of the different terms are independent of one another\n",
        "  - P(A|B) = P(B|A) x P(A) / P(B)\n",
        "  - Posterior = prior x likelihood / evidence\n",
        "  - Laplace correction : A smoothing technique to avoid frequency based 0 prob.(sparse matrix) A small sample correction (pseudo-count alpha is added)\n",
        "    - theta(i) = xi + alpha / N + (alpha X d)\n",
        "\n",
        "### Evaluation:\n",
        "\n",
        "- Cross Validation : Model validation tech\n",
        "  - Help us evaluate the quality of the model\n",
        "  - To select the model that will perfrom best on unseen data\n",
        "  - to avoid overfitting and underfitting the data\n",
        "  - test / train split or K-fold validation (k=5, generally)\n",
        "\n"
      ],
      "metadata": {
        "id": "xAs-e91wfhjB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4hrB8xtdOtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large language models:\n",
        "\n",
        "- Building Blocks:\n",
        "  - Text pre-processing : raw text data into std format, Tokenization -> Stop Word Removal -> Lemmatization\n",
        "\n",
        "  - Text representation :\n",
        "    - BOW (matrix of word counts)\n",
        "    -  Word embeddings (semantic representation -> word weights -> relationship modeling)\n",
        "  - Pre-Training\n",
        "  - Fine-Tuning:\n",
        "    - Transfer learning : (N-shot)\n",
        "      - Zero-shot : no task specific data\n",
        "      - Few-shot : little task specific data\n",
        "      - Multi-shot : relatively more training data\n",
        "\n",
        "\n",
        "## Pretraining to build LLMs:\n",
        "\n",
        "- Generative Pre-Training:\n",
        "  - Input data of text tokens -> trained to predict the tokens within the dataset\n",
        "    - Next Word Prediction :\n",
        "      - Supervised learning (trained on Input / output pairs)\n",
        "      - predicts next word, generated coherent text\n",
        "    - Masked Language modeling:\n",
        "      - Hides a selective word\n",
        "      - trained model predicts the masked word\n",
        "\n",
        "## Transformer:\n",
        "\n",
        "- Part of pre training\n",
        "- Attention is all you need\n",
        "\n",
        "- Architecture:\n",
        "  - Long range relaiton ship between the words to generate coherent text\n",
        "  - Components:\n",
        "    - Pre-processing\n",
        "    - Positinal encoding : understanding distant words\n",
        "    - encoders : attention mechanism and neural network\n",
        "    - decoders\n",
        "  - Challenge: Long range dependency\n",
        "  - attention: focus on different parts of the input\n",
        "\n",
        "  - Tranformers process mutiple part simultaneously\n",
        "  - Faster processing\n",
        "\n",
        "### Attention mechanisms:\n",
        "\n",
        "- Understand complex structures\n",
        "- focus on important words\n",
        "  - Self attention:\n",
        "    - weighs importance of each word\n",
        "    - captures long range dependencies\n",
        "    - in a group conversation, evaluating each person's words and comparing their relevance\n",
        "    - combine for a more comprehensive understanding for the conversation\n",
        "  \n",
        "\n",
        "  - Muti-head\n",
        "    - Splits the input into multiple heads focusing on different aspects of the relationships between words\n",
        "    - for instance: different aspects of the conversation, speaker's emotion, primary topic, related side-topic\n",
        "\n",
        "    - The boy wnet to the store to buy some groceries and he found a discount on his favourite cereal\n",
        "      - attention: boy, store, groceries, discount\n",
        "      - self-attention: boy, he - same person\n",
        "      - multi-head : character, Action, things involved\n",
        "\n",
        "### Advanced Fine tuning:\n",
        "\n",
        "- Reinforcement learning through Human feedback:\n",
        "  - General purpose data lacks quality : noise, errors, reduced accuracy\n",
        "  - Reviewed by human\n",
        "\n",
        "### Data concerns / considerations\n",
        "\n",
        "1. Data volume\n",
        "2. Data quality (Labeled data)\n",
        "3. Data Bias: Societal stereotypes, evaluate bias mitigation techniques\n",
        "4. Privacy\n",
        "\n",
        "## check LLM\n",
        "---\n"
      ],
      "metadata": {
        "id": "LmPswXyYdQOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 6 :\n",
        "\n",
        "## 1. Topic Modeling\n",
        "\n",
        "- Intro to topic modeling:\n",
        "  - BOW : numeric frequencies of the words\n",
        "  - Document term Matrix :\n",
        "    - Frequency of terms in a collection of documents\n",
        "    - each row represents a document\n",
        "    - Sparse, Cannot capture Latent variables (semantics)\n",
        "\n",
        "- Topic Modeling : The process of learning, recognizing, and extracting hidden topics accross a collection of documents\n",
        "  - Idea: Each docmuent consists of a mixture of topics, each topic consists of a collection of words\n",
        "  - Types:\n",
        "    1. LSI(LSA): Latent semantic indexing\n",
        "    2. pLSA: probablistic latent semantic analysis\n",
        "    3. LDA: Latent Dirichlet allocation\n",
        "    4. lda2vec: LDA + Word2vec\n",
        "\n",
        "- **Latent Semantic Analysis**:\n",
        "  - oldest / simplest\n",
        "  - reducing the word space dimensionality\n",
        "  - Representation:\n",
        "    - Term Document Matrix (TDM / DTM)\n",
        "    - The matrix is decomposed using SVD (singular value decomposition)\n",
        "      - Find the best approximation of the data points using fewer dimensions\n",
        "      - Identify and order the dimensions along which datapoints exhibit the most variation\n",
        "      - [A] = [U][S][V]^T\n",
        "        - U is the left singular vector of words (relation between documents and topics)\n",
        "        - S weights on the diagonal (scales the matrix by their corpus strength)\n",
        "        - V is the right singular vector of documents (Models the term 'Topic relationship')\n",
        "      - **Truncated SVD** : Reduces dimensionality by selecting only:\n",
        "        - K largest S values\n",
        "        - only K columns of U and V (K is the hyper-parameter that we can adjust to select the nnumber of topics we want to find)\n",
        "        - see the LDA code\n",
        "  - Strength : Noise removal, dimensionality reduction, captures semantic relation\n",
        "  - Weakness : Interpretability (topics are word vectors with both, positive and negative direction), Evaluation\n",
        "\n",
        "- **Probablistic Latent Semantic Analysis**:\n",
        "  - probablistic method instead of SVD\n",
        "  - probablity of a word W appearing in a document D as a mixture of conditionally independent multinomial distribution (trained via expextation maximization algorithm as P(Z is hidden) that involved topics\n",
        "  - Core Idea: to find a probablistic model with latent topics that can generate the data we observe in the TDM matrix\n",
        "  - Hyperparamter : Number of topics\n",
        "\n",
        "  - Formula : P(D,W) = P(D) Sigma(Z) P(Z|D) P(W|Z)\n",
        "  - d-> document, Z->Topic, W->Word\n",
        "  - Strength: Models can be compared using the probablities assigned to the new documents, represented with positve topic assignment\n",
        "  - Weakness: Computational complexity, does not yeild a generative model for other documents\n",
        "\n",
        "  - Implementation: Equivalent to Non-Negative Matrix Factorization using 'beta-loss='kullback-leibler'' convergence\n",
        "\n",
        "- **Latent Dirichlet Allocation (LDA)**:\n",
        "  - Extends pLSA by adding a generative process\n",
        "  - Is a generative model, produces hierarchical bayesian model:\n",
        "    - Assumptions:\n",
        "      - topics are probablity distribution over words\n",
        "      - documents are probability over topics\n",
        "      - topics follow sparse dirichlet distribution\n",
        "  - Also has a variant to include metadata (authors,imagedata,etc)\n",
        "  - Process:\n",
        "    - We take a random sample of topics of a particular document with dirichlet distribution\n",
        "\n",
        "\n",
        "## Advanced topic modeling : Neural relational topic models\n",
        "\n",
        "https://www.youtube.com/watch?v=ykk-FUoDt74\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wETGODuDo6I8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iq5xM_kAdT-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Sentiment analysis with python\n",
        "\n",
        "Sentiment analyis : opinion mining\n",
        "\n",
        "3 elements:\n",
        "  - opinion (polarity : positive, neutral, negative) / emotion (joy, surprise, disgust)\n",
        "  - Subject\n",
        "  - Opinion holder / entity holding the opinion\n",
        "\n",
        "Used In\n",
        "  - Social media monitoring\n",
        "  - Brand Monitoring\n",
        "  - Customer service\n",
        "  - Market research / Analysis\n",
        "\n",
        "\n",
        "1. Types and approaches:\n",
        "  - Levels of granularity:\n",
        "    1. Document level (whole review)\n",
        "    2. Sentence level (opinion in each sentence)\n",
        "    3. Aspect level (different features of the product)\n",
        "\n",
        "  - Types of algorithms:\n",
        "    - Rule / lexicon based (nice +2, good+1,etc.) : Matches the words in the lexicon, then averages or sums the total (total valence)\n",
        "      - relies on dictionaries\n",
        "      - different words might have different polarity in different context\n",
        "      - fast\n",
        "\n",
        "    - Automatic / Machine learning : using historical data, predicting the sentiment of a new piece of text\n",
        "      - relies on historical data\n",
        "      - takes time to train\n",
        "\n",
        "    - Hybrid is the best (mostly)\n",
        "\n",
        "              Calculating the total Valence:\n",
        "\n",
        "              from textblob import TextBlob\n",
        "\n",
        "              # returns a tuple (polarity(-1,1),subjectivity(0,1))\n",
        "              my_valence = TextBlob(text)\n",
        "              my_valence.sentiment\n",
        "  \n",
        "2. Word Cloud:\n",
        " - Size: corresponds to the frequency of the word\n",
        "\n",
        "          from wordcloud import WordCloud\n",
        "          import matplotlib.pyplot as plt\n",
        "\n",
        "          # To see all functions:\n",
        "          ?WordCloud\n",
        "\n",
        "          my_cloud = WordCloud(background_color='white', stopwords=my_stopwords).generate(text)\n",
        "\n",
        "          # can also specify to remove stopwords\n",
        "\n",
        "          plt.imshow(cloud_two, interpolation = 'bilinear')\n",
        "\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "3. Bag-of-Words:\n",
        "\n",
        "       import pandas as pd\n",
        "       from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "       vect = CountVectorizer(max_features=1000)\n",
        "       vect.fit(data.review)\n",
        "       X = vect.transform(data.review)\n",
        "\n",
        "       # creates a sparse matrix, to view need to convert to a dense array\n",
        "\n",
        "       my_array = X.toarray()\n",
        "       X_df = pd.DataFrame(my_array, columns=vect.get_feature_names())\n",
        "\n",
        "- N grams: Context matters, Unigrams, BIgrams, TRIgrams, N-grams\n",
        "  - Use grid search to find the best model to fit as risk of overfitting increases\n",
        "  - use max_features to define the length of the vocab\n",
        "\n",
        "        vect = CountVectorizer(ngram_range=(min_n, max_n))\n",
        "\n",
        "        # vocab size:\n",
        "        CountVectorizer(max_features,\n",
        "         max_df(ignore terms with higher than specified freq),\n",
        "         min_df(can be integer, float))\n",
        "\n",
        "- Building new features from Text: Enriching the dataset with a sentiment\n",
        "  - Tokenizing a string\n",
        "\n",
        "        from nltk import word_tokenize\n",
        "        anna_k = text\n",
        "        word_tokenize(anna_k)\n",
        "\n",
        "        # tokens from a column\n",
        "\n",
        "        word_tokens = [word_tokenize(review) for review in reviews.review]\n",
        "\n",
        "        len_tokens = []\n",
        "\n",
        "        for i in range(len(word_tokens[i])):\n",
        "          len_tokens.append(len(word_tokens[i]))\n",
        "\n",
        "        new_column['n_tokens'] = len_tokens\n",
        "\n",
        "        # punctuation signs can tell how emotionally charged a review is\n",
        "\n",
        "- Guessing the language:\n",
        "\n",
        "      from langdetect import detect_langs\n",
        "      foreign = 'dfdfb'\n",
        "\n",
        "      detect_langs(foreign)\n",
        "\n",
        "      # returns a list of languages\n",
        "\n",
        "      for row in range(len(reviews)):\n",
        "        languages.append(detect_langs(reviews.iloc[row,1]))\n",
        "\n",
        "      languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
        "\n",
        "- Stop Words:\n",
        "  - occur too frequently and are not informative\n",
        "  - from wordclouds\n",
        "\n",
        "        from wordcloud import WordCloud, STOPWORDS\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # define the stopwrods list\n",
        "        my_stopwords = set(STOPWORDS)\n",
        "        my_stopwords.update(['movie','movies'])\n",
        "\n",
        "        my_cloud = WordCloud(backgrouund_color='white',stopwords=my_stopwords).generate(name_string)\n",
        "\n",
        "  - from BOW:\n",
        "        from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "\n",
        "        # defining the set of stopwords\n",
        "        my_stop_words = ENGLISH_STOP_WORDS.union([mine])\n",
        "\n",
        "        vect = CountVectorizer(stop_words= my_stop_words)\n",
        "        vect.fit(movies.review)\n",
        "        vect.transform(movies.review)\n",
        "\n",
        "  - Capturing a Token Pattern\n",
        "    - my_string.isalpha()\n",
        "    - .isdigit()\n",
        "    - .isalnum()\n",
        "    - regex\n",
        "          cleaned tokens = [[word for word in item if word.isapha()]for item in words_tokens]\n",
        "\n",
        "          vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(tweets.text)\n",
        "  \n",
        "  - stemming and lemmatization:\n",
        "    - stemming: words to root forms even if the stem is not valid in the root language\n",
        "          # stemming:\n",
        "\n",
        "          from nltk.stem import PorterStemmer\n",
        "          porter = PorterStemmer()\n",
        "\n",
        "          porter.stem('wonderful')\n",
        "\n",
        "          # other languages\n",
        "\n",
        "          from nltk.stem.snowball import SnowballStemmer\n",
        "          dutchstemmer = SnowballStemmer('dutch')\n",
        "\n",
        "          dutchstemmer.stem(word)\n",
        "\n",
        "          # stemming can only be done on words\n",
        "          \n",
        "          tokens = word_tokenize(text)\n",
        "          stemmed_tokens = [porter.stem(token) for token in tokens]\n",
        "\n",
        "    - Lemma: valid roots, requires a pos\n",
        "          # lemmatizer\n",
        "\n",
        "          from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "          WNlemmatizer = WordNetLemmatizer()\n",
        "          WNlemmatizer.lemmatize(word, pos='a')\n",
        "\n",
        "  - TFIdf:\n",
        "    - automatically penalizes stopwords\n",
        "    - also producues a sparse matrix initially (only non zero values)\n",
        "\n",
        "          from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "          # arguments = max_features, ngram_range, stop_words, token_pattern, max_df, min_df\n",
        "\n",
        "          vect = TfidfVectorizer(mex_features=100).fit(Tweets.text)\n",
        "          X = vect.transform(tweets.text)\n",
        "\n",
        "          x_df = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
        "\n",
        "\n",
        "3. Final sentiment prediction using machine learning:\n",
        "\n",
        "  - Classification Problem\n",
        "    - Logistic Regression:\n",
        "      - Sigmoid function (0,1)\n",
        "      - Probability (sentiment = positive|review)\n",
        "             from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "             log_reg = LogisticRegression().fit(X,y)\n",
        "\n",
        "             # Model performance : accuracy\n",
        "\n",
        "             score = log_reg.score(X,y)\n",
        "\n",
        "             # score gives different metrics for different models\n",
        "\n",
        "             from sklearn.metrics import accuracy_score\n",
        "             y_predicted = log_reg.predict(X)\n",
        "             accuracy = accuracy_score(y,y_predicted)\n",
        "\n",
        "      - Train_test split, Confusion Matrix\n",
        "             From sklearn.model_selection import train_test_split\n",
        "\n",
        "             X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=0.2, stratify=y)\n",
        "\n",
        "             # stratify=y, proportion same as the given column in split\n",
        "\n",
        "             # confusion matrix\n",
        "             [True +, False +]\n",
        "             [False -, True -]\n",
        "\n",
        "             # from sklearn.metrics import confusion_matrix\n",
        "\n",
        "             print(confusion_matrix(y_test, y_predicted)/len(y_test))\n",
        "             \n",
        "      - Complex models that captures the noise in the datset leads to overfitting\n",
        "\n",
        "      - Regularization: way to penalise the models\n",
        "        - applied by delfault\n",
        "        - uses L2 penalty : shrinks all the coeffs towards 0, higher C-> less regularization\n",
        "               LogisticRegression(penalty='l2',C=1.0)\n",
        "\n",
        "      - Predicting the probability rather than the class:\n",
        "            \n",
        "             y_probab = log_reg.predict_proba(X_test)\n",
        "             \n",
        "             # produces an array of probabilites with prob of each class, i.e 0,1,2\n",
        "\n",
        "             # default is 0.5 split, however, the probability threshold should depend on the proportion of classes in data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "drR59UWlTB8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datacamp 4: Spoken language processing\n",
        "\n",
        "## Introduction to audio data in python:\n",
        "- processing audio files:\n",
        "      import wave\n",
        "      good_morning = wave.open('good-morning.wav','r')\n",
        "\n",
        "      framerate_gm = good_morning.getframerate()\n",
        "\n",
        "      # audiofile duration:\n",
        "      duration = len(good_morning)/framerate_gm\n",
        "\n",
        "      # convert wave to bytes:\n",
        "      soundawave_gm = good_morning.readframes(-1)\n",
        "\n",
        "      # from byte to integers\n",
        "      import numpy to np\n",
        "      \n",
        "      signal_gm = np.frombuffer(soundwave_gm, dtype='int16')\n",
        "\n",
        "      #Finding Soundwave stames : np.linspace gives equally spaced values between start and stop\n",
        "\n",
        "      time_gm = np.linspace(start=0, stop=len(soundwave_gm)/framerate_gm, num=len(soundwave_gm))\n",
        "\n",
        "- Visualizing soundwaves:\n",
        "      import matplot.pyplot as plt\n",
        "\n",
        "      plt.title('Good afternoon vs good morning')\n",
        "      plt.xlabel('Time(seconds)')\n",
        "      plt.ylabel('Amplitude')\n",
        "\n",
        "      plt.plot('time_ga',soundwave_ga, label='Good Afternoon')\n",
        "      plt.plot(time_gm, soundwave_gm, label='Good Morning', alpha=0.5)\n",
        "\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "- Speech recognition python libraries:(CMU Spinx, Kaldi, speechRecognition)\n",
        "      # Installing the library\n",
        "      pip install SpeechRecognition\n",
        "\n",
        "      # SpeechRecognition\n",
        "      import speech_recognition as sr\n",
        "      recognizer = sr.Recognizer()\n",
        "\n",
        "      # energy threshold (silent=100)\n",
        "      recognizer.energy_threshold = 300\n",
        "\n",
        "      # using api's to convert the audio file to text:\n",
        "      recognize_bing, recognize_google, recognize_google, recognize_google_cloud\n",
        "\n",
        "      # transcribe using API:\n",
        "      recognizer.recognize_google(audio_data=audio_file, lang='en-US')\n",
        "\n",
        "- Audio files direclty using wav is saved as  audiofile and needs to be converted to audiodata to be used by recognize_google()\n",
        "\n",
        "      # convert from audiofile to audiodata\n",
        "      with clean_support_call as source:\n",
        "        clean_support_call_audio = recognizer.record(source)\n",
        "\n",
        "      recognizer.recognize_google(audio_data=clean_support_call_audio)\n",
        "\n",
        "      # record parameters:\n",
        "\n",
        "      recognizer.record(source, duration = (2.0 in seconds), offeset = 5.0)\n",
        "\n",
        "- Different kinds of audio:\n",
        "      # show all parameter\n",
        "\n",
        "      with leopard_roar as source:\n",
        "        leopard_roar_audio = recognizer.record(source)\n",
        "\n",
        "      recognizer.recognize_google(leopard_roar_audio, show_all=True)\n",
        "\n",
        "      # will give a list of all the possible audios\n",
        "\n",
        "      # multiple audio files:\n",
        "\n",
        "      speakers = [sr.audiofile('s1.wav'),sr.audiofile('s2.wav')]\n",
        "\n",
        "      for i, speaker in enumerate(speakers):\n",
        "        with speaker as source:\n",
        "          speaker_audio = recognizer.record(source)\n",
        "\n",
        "- Noisy audio:\n",
        "      # using -> adjust_for_ambient_noise(source, duration)\n",
        "\n",
        "      with noisy_support_call as source:\n",
        "        recognizer.adjust_for_ambient(source, duration=0.5)\n",
        "      \n",
        "      noisy_support_call_audio = recognizer.record(source)\n",
        "\n",
        "- Pydub\n",
        "      # pip install pydub : works for wav\n",
        "      - for mp3: ffmpeg via ffmpeg.org\n",
        "\n",
        "      from pydub import AudioSegment\n",
        "\n",
        "      wav_file = AudioSegment.from_file(file='.wav', format='wav')\n",
        "\n",
        "      - creates a pydup.audiosegment file\n",
        "\n",
        "      # Playing a wav file:\n",
        "\n",
        "      pip install simpleaudio\n",
        "\n",
        "      from pydub.playback import play\n",
        "\n",
        "      play(wav_file)\n",
        "\n",
        "      # Audio parameters:\n",
        "\n",
        "      wav_file.channels (mono,stereo)\n",
        "      wav_file.frame_rate\n",
        "      wav_file.sample_width(no of bytes)\n",
        "      wav_file.max(maximum amplitude)\n",
        "      len(wav_file): length of audiofile in milliseconds\n",
        "\n",
        "      # changing parameters:\n",
        "\n",
        "      .set_sample_width(1)\n",
        "      .set_frame_rate(16000)\n",
        "      .set_channels(1)\n",
        "\n",
        "      # increasing the volume of audio segments:\n",
        "      louder_wav_file = wav_file + 10\n",
        "\n",
        "- Normalising the audio:\n",
        "      from pydub import AudioSegment\n",
        "      from pydub.effects import normalise\n",
        "      from pydub.playback import play\n",
        "\n",
        "      - to either boost or reduce audio levels to match the audio levels of the entire clip\n",
        "\n",
        "      loud_quiet = AudioSegment.from_file('loud_quiet.wav')\n",
        "      normalised = normalize(loud_quiet)\n",
        "\n",
        "      # Remixing audio segments\n",
        "\n",
        "      removing static as time is measured in ms\n",
        "\n",
        "      no_static = static_check[5000:]\n",
        "\n",
        "      wave_3 = wav_2 + wav_1\n",
        "\n",
        "      # combining scales the parameters to higher quality audiofile\n",
        "      # Splitting audio from stereo to mono\n",
        "\n",
        "      phono_call_channels = phone_call.split_to_mono()\n",
        "\n",
        "      phone_call_channels[0], phone_call_channels[1]\n",
        "\n",
        "      # converting and exporting audio signals\n",
        "\n",
        "      louder_wav_file.export(out_f='.wav', format='wav')\n",
        "\n",
        "- Reformatting mutiple audio files\n",
        "      def make_wav(wrong_folder_path, right_folder_path):\n",
        "      # Loop through wrongly formatted files:\n",
        "\n",
        "      for file in os.scandir(wrong_folder_path):\n",
        "\n",
        "        if file.path.endswith('.mp3') or file.path.endswith('.flac'):\n",
        "          out_file = right_folder_path + os.path.splitext(os.path.basename(file.path))[0] + '.wav'\n",
        "\n",
        "      AudioSegment.from_file(file.path).export(out_file, format='wav')\n",
        "\n",
        "- Spoken Launguage processing pipeline:\n",
        "      def convert_to_wav(filename):\n",
        "      \"\"\"Takes an audio file of non .wav format and converts to .wav\"\"\"\n",
        "      \n",
        "      # Import audio file\n",
        "      audio = AudioSegment.from_file(filename)\n",
        "      \n",
        "      # Create new filename\n",
        "      new_filename = filename.split(\".\")[0] + \".wav\"\n",
        "      \n",
        "      # Export file as .wav\n",
        "      audio.export(new_filename, format='wav')\n",
        "      print(f\"Converting {filename} to {new_filename}...\")\n",
        "      \n",
        "      # Test the function\n",
        "      convert_to_wav('call_1.mp3')\n",
        "\n",
        "\n",
        "      def show_pydub_stats(filename):\n",
        "      \n",
        "      \"\"\"Returns different audio attributes related to an audio file.\"\"\"\n",
        "      # Create AudioSegment instance\n",
        "      audio_segment = AudioSegment.from_file(filename)\n",
        "      \n",
        "      # Print audio attributes and return AudioSegment instance\n",
        "      print(f\"Channels: {audio_segment.channels}\")\n",
        "      print(f\"Sample width: {audio_segment.sample_width}\")\n",
        "      print(f\"Frame rate (sample rate): {audio_segment.frame_rate}\")\n",
        "      print(f\"Frame width: {audio_segment.frame_width}\")\n",
        "      print(f\"Length (ms): {len(audio_segment)}\")\n",
        "      return audio_segment\n",
        "\n",
        "      # Try the function\n",
        "      call_1_audio_segment = show_pydub_stats('call_1.wav')\n",
        "\n",
        "\n",
        "      def transcribe_audio(filename):\n",
        "      \"\"\"Takes a .wav format audio file and transcribes it to text.\"\"\"\n",
        "      # Setup a recognizer instance\n",
        "      recognizer = sr.Recognizer()\n",
        "      \n",
        "      # Import the audio file and convert to audio data\n",
        "      audio_file = sr.AudioFile(filename)\n",
        "      with audio_file as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "      \n",
        "      # Return the transcribed text\n",
        "      return recognizer.recognize_google(audio_data)\n",
        "\n",
        "      # Test the function\n",
        "      print(transcribe_audio('call_1.wav'))\n",
        "\n",
        "- Sentiment analysis on spoken language\n",
        "      pip install nltk\n",
        "\n",
        "      import nltk\n",
        "      nltk.download('punkt')\n",
        "      nltk.download('vader_lexicon')\n",
        "\n",
        "      # import sentiment analysis class\n",
        "\n",
        "      from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "      # create sentiment analysis instance\n",
        "      sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "      print(sid.polarity_scores('text'))\n",
        "\n",
        "      from nltk.tokenize import sent_tokenize\n",
        "\n",
        "      for sentence in sent_tokenize(text):\n",
        "        print(sentence)\n",
        "        print(sid.polarity_scores(sentence))\n",
        "\n",
        "- Named Entity recognition transcribed text\n",
        "\n",
        "      nlp = spacy.load('en_core_web_sm')\n",
        "      doc = nlp('text')\n",
        "\n",
        "      for sentences in doc.sents:\n",
        "        print(sents)\n",
        "\n",
        "      for entity in doc.ents:\n",
        "        print(entity.text, entity.label_)\n",
        "\n",
        "      - custom named entities:\n",
        "\n",
        "      from spacy.pipeline import EntityRuler\n",
        "      print(nlp.pipeline)\n",
        "\n",
        "- Classifying transcribed speech using sklearn:\n",
        "\n",
        "      import os\n",
        "      post_purchase_audio = os.listdir('post_purchase')\n",
        "\n",
        "      # Build the text_classifier as an sklearn pipeline\n",
        "      text_classifier = Pipeline([\n",
        "          ('vectorizer', CountVectorizer()),\n",
        "          ('tfidf', TfidfTransformer),\n",
        "          ('classifier', MultinomialNB()),\n",
        "      ])\n",
        "\n",
        "      # Fit the classifier pipeline on the training data\n",
        "      text_classifier.fit(train_df.text, train_df.label)\n"
      ],
      "metadata": {
        "id": "sEu3MKGRdeJK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIjPmQxB-EKx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}